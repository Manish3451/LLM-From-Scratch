# ðŸ§  Building a Large Language Model (LLM) from Scratch

<p align="center">
  <img src="images/llm.png" alt="LLM Project Banner" width="800"/>
</p>

---

## ðŸ“˜ Overview

This project implements a complete pipeline for **building, training, and fine-tuning a GPT-2 style language model** from scratch using **PyTorch**. It covers all key stages â€” from data preparation and tokenization to transformer architecture, pretraining, and downstream fine-tuning for tasks like classification.

This implementation is inspired by the book *Build a Large Language Model (from Scratch)* by **Raschka, Mirjalili, and D'Souza (2024)**.

---

## ðŸ“‚ Project Structure

```
LLM FROM SCRATCH/
â”œâ”€â”€ pycache/
â”œâ”€â”€ gpt2/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ llm.png
â”‚   â””â”€â”€ Screenshot 2025-10-21 163332.png
â”œâ”€â”€ llm/
â”œâ”€â”€ sms_spam_collection/
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”œâ”€â”€ gpt_download3.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ LLM.tokenizer.ipynb
â”œâ”€â”€ loss-plot.pdf
â”œâ”€â”€ model_and_optimizer.pth
â”œâ”€â”€ model.pth
â”œâ”€â”€ README.md
â”œâ”€â”€ sms_spam_collection.zip
â”œâ”€â”€ temperature-plot.pdf
â”œâ”€â”€ test.csv
â”œâ”€â”€ the-verdict.txt
â”œâ”€â”€ train.csv
â”œâ”€â”€ validation.csv
â””â”€â”€ verdict.txt
```

---

## ðŸ”„ Three-Stage Training Pipeline

The project follows a systematic **three-stage pipeline** to build and deploy a production-ready GPT-2 style model using transfer learning.

### **Stage 1: Building the LLM**

1. **Data Preparation & Sampling**

   * Tokenization of raw text using OpenAIâ€™s `tiktoken` library (GPT-2 BPE encoder).
   * Creation of input-target pairs using a sliding window.
   * Custom PyTorch DataLoader for efficient batching.

2. **Attention Mechanism**

   * Causal multi-head self-attention implementation.
   * Scaled dot-product attention with causal masking.
   * Multiple attention heads capture semantic relationships.

3. **LLM Architecture**

   * Transformer model with embedding layers.
   * Stacked transformer blocks with attention & feed-forward layers.
   * Layer normalization and residual connections for training stability.

---

### **Stage 2: Pretraining the Foundation Model**

4. **Pretraining**

   * Train on *the-verdict.txt* corpus using next-token prediction.
   * Learns language structure, grammar, and semantics.

5. **Training Loop**

   * Implements PyTorch-based optimization and loss calculation.
   * AdamW optimizer with gradient clipping and scheduler.

6. **Model Evaluation**

   * Validation on held-out data for convergence tracking.
   * Checkpointing to save progress.

---

### **Stage 3: Fine-Tuning for Downstream Tasks**

7. **Load Pretrained Weights**

   * Load official GPT-2 (124M) weights automatically via `gpt_download3.py`.

8. **Fine-Tuning for Classification**

   * Train on **SMS Spam Collection** dataset for spam detection.
   * Replace LM head with a classification head.
   * Freeze most transformer layers for efficient adaptation.

9. **Fine-Tuning for Chat Tasks**

   * Adaptation for conversational datasets (instruction-following).
   * Model learns context retention and response generation.

---

## ðŸ§± Model Architecture

GPT-2 Small (124M) Configuration:

| Parameter        | Value |
| ---------------- | ----- |
| `vocab_size`     | 50257 |
| `context_length` | 1024  |
| `emb_dim`        | 768   |
| `n_heads`        | 12    |
| `n_layers`       | 12    |
| `drop_rate`      | 0.1   |
| `qkv_bias`       | False |

### **Core Components**

* **Token & Position Embeddings:** Map tokens and sequence positions to dense vectors.
* **Transformer Blocks:** Multi-head self-attention + MLP layers with GELU activation.
* **Residuals & LayerNorm:** Ensure gradient stability and fast convergence.

---

## ðŸ“ˆ Results and Performance

### **Pretraining Loss**

* Training loss decreases from ~9.5 â†’ <1.0 in 10 epochs.
* Validation loss stabilizes around 6.5.
* Clear convergence after epoch 6.

### **Fine-Tuning Performance**

| Metric              | Value     |
| ------------------- | --------- |
| Training Accuracy   | **100%**  |
| Validation Accuracy | **97.5%** |

### **Text Generation with Temperature Control**

* Supports *temperature scaling* and *top-k sampling* for output control.

---

## âš™ï¸ Usage

### **Environment Setup**

```bash
pip install torch tiktoken pandas matplotlib tensorflow tqdm
```

### **Download Pretrained Weights**

```bash
python gpt_download3.py
```

### **Training from Scratch**

```bash
jupyter notebook LLM.tokenizer.ipynb
```

Covers:

* Data preparation & tokenization
* Model architecture implementation
* Pretraining
* Fine-tuning for spam detection
* Text generation

### **Fine-Tuning for Custom Tasks**

```python
# Load pretrained model
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(torch.load('model.pth'))

# Add custom classification head
num_classes = your_num_classes
model.out_head = torch.nn.Linear(
    in_features=GPT_CONFIG_124M["emb_dim"],
    out_features=num_classes
)

# Fine-tune on your dataset
train_classifier(model, your_train_loader, your_val_loader)
```

---

## ðŸŒŸ Key Features

* âœ… Complete Implementation from Scratch
* ðŸ§© Modular & Extensible Design
* ðŸ”„ Pretrained Weight Compatibility
* ðŸ§  Multi-Task Fine-Tuning
* âš¡ Parameter-Efficient Training
* âœ¨ Flexible Text Generation
* ðŸ§‘â€ðŸ’» Production-Ready Code

---

## ðŸ§® Technical Highlights

| Component                 | Description                           | Benefit                           |
| ------------------------- | ------------------------------------- | --------------------------------- |
| **Attention Mechanism**   | Causal masking + multi-head attention | Enables autoregressive generation |
| **Training Optimization** | AdamW + learning rate scheduling      | Stable convergence                |
| **Memory Efficiency**     | Batch processing + gradient clipping  | Handles large batches safely      |

---

## ðŸš€ Future Enhancements

* GPT-2 Medium / Large / XL Variants
* Multi-GPU Distributed Training
* Advanced Decoding (Nucleus, Beam Search)
* Integration with Instruction-Tuning Datasets
* RLHF Pipeline (Reinforcement Learning from Human Feedback)
* Model Quantization for Lightweight Deployment

---

## ðŸ“š References

* Raschka, S., Mirjalili, V., & D'Souza, D. (2024). *Build a Large Language Model (from Scratch)*. Manning.
* Vaswani, A., et al. (2017). *Attention Is All You Need.*
* Radford, A., et al. (2019). *Language Models are Unsupervised Multitask Learners.*
* SMS Spam Collection Dataset Contributors.

---

## ðŸªª License

This project is licensed under the terms specified in the `LICENSE` file.

---

## ðŸ™Œ Acknowledgments

* OpenAI â€” GPT-2 architecture and pretrained weights
* Sebastian Raschka â€” Educational materials and guidance
* PyTorch Team â€” Deep learning framework
* SMS Spam Collection Dataset Contributors
