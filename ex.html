<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Gradient Descent Optimizers: From SGD to Adam</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- JavaScript Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script>
        // MathJax Configuration
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Inter', sans-serif;
            background: #0f1419;
            color: #c9d1d9;
            line-height: 1.7;
        }
        
        code {
            font-family: 'Fira Code', monospace;
        }
        
        .accent-blue { color: #58a6ff; }
        .accent-muted { color: #8b949e; }
        .bg-card { 
            background: rgba(22, 27, 34, 0.8);
            backdrop-filter: blur(8px);
            border: 1px solid rgba(48, 54, 61, 0.8);
        }
        
        /* Subtle animated background */
        .animated-bg {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            background: radial-gradient(ellipse at 20% 30%, rgba(88, 166, 255, 0.03) 0%, transparent 50%),
                        radial-gradient(ellipse at 80% 70%, rgba(163, 182, 138, 0.03) 0%, transparent 50%);
        }
        
        /* Navbar */
        .navbar {
            backdrop-filter: blur(12px);
            background: rgba(13, 17, 23, 0.85);
            border-bottom: 1px solid rgba(48, 54, 61, 0.5);
        }
        
        .nav-link {
            position: relative;
            transition: color 0.2s ease;
            font-size: 0.9rem;
            letter-spacing: 0.3px;
        }
        
        .nav-link::after {
            content: '';
            position: absolute;
            bottom: -2px;
            left: 0;
            width: 0;
            height: 1px;
            background: #58a6ff;
            transition: width 0.2s ease;
        }
        
        .nav-link:hover {
            color: #58a6ff;
        }
        
        .nav-link:hover::after {
            width: 100%;
        }

        /* Neural Network Canvas */
        #neuralNetCanvas {
            width: 100%;
            height: 380px;
            border-radius: 8px;
            background: rgba(13, 17, 23, 0.6);
            border: 1px solid rgba(48, 54, 61, 0.5);
        }

        /* Code blocks */
        pre {
            background: rgba(13, 17, 23, 0.8);
            border-radius: 6px;
            padding: 20px;
            overflow-x: auto;
            position: relative;
            border: 1px solid rgba(48, 54, 61, 0.6);
        }
        
        code {
            font-size: 0.875rem;
            line-height: 1.6;
            color: #c9d1d9;
        }
        
        .copy-btn {
            position: absolute;
            top: 12px;
            right: 12px;
            background: rgba(48, 54, 61, 0.8);
            color: #c9d1d9;
            border: 1px solid rgba(88, 166, 255, 0.3);
            padding: 6px 12px;
            border-radius: 4px;
            cursor: pointer;
            transition: all 0.2s ease;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .copy-btn:hover {
            background: rgba(88, 166, 255, 0.15);
            border-color: rgba(88, 166, 255, 0.5);
        }
        
        /* Details/Summary */
        details > summary {
            cursor: pointer;
            font-weight: 500;
            padding: 14px 18px;
            background: rgba(22, 27, 34, 0.6);
            border-radius: 6px;
            border: 1px solid rgba(48, 54, 61, 0.6);
            transition: all 0.2s ease;
            list-style: none;
        }
        
        details > summary::-webkit-details-marker {
            display: none;
        }
        
        details > summary::before {
            content: '▶';
            display: inline-block;
            margin-right: 8px;
            transition: transform 0.2s ease;
            color: #58a6ff;
            font-size: 0.8rem;
        }
        
        details[open] > summary::before {
            transform: rotate(90deg);
        }
        
        details > summary:hover {
            background: rgba(22, 27, 34, 0.8);
            border-color: rgba(88, 166, 255, 0.3);
        }

        /* Fade animations */
        .fade-in-section {
            opacity: 0;
            transform: translateY(20px);
            transition: opacity 0.5s ease, transform 0.5s ease;
        }
        
        .fade-in-section.is-visible {
            opacity: 1;
            transform: translateY(0);
        }

        /* Table */
        .custom-table {
            display: grid;
            grid-template-columns: 1fr 1.5fr 1.5fr 1fr;
            gap: 1px;
            background: rgba(48, 54, 61, 0.5);
            border-radius: 6px;
            overflow: hidden;
        }
        
        .table-cell {
            background: rgba(22, 27, 34, 0.8);
            padding: 16px;
            transition: background 0.2s ease;
        }
        
        .table-cell:hover {
            background: rgba(22, 27, 34, 0.95);
        }
        
        .table-header {
            background: rgba(30, 36, 44, 0.9);
            font-weight: 600;
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: #8b949e;
        }
        
        @media (max-width: 768px) {
            .custom-table {
                grid-template-columns: 1fr;
            }
            .table-header { display: none; }
            .table-cell {
                display: grid;
                grid-template-columns: 110px 1fr;
                gap: 12px;
            }
            .table-cell::before {
                content: attr(data-label);
                font-weight: 600;
                color: #58a6ff;
            }
        }

        /* Explanation boxes */
        .explanation-box {
            background: rgba(22, 27, 34, 0.5);
            border-left: 3px solid rgba(88, 166, 255, 0.5);
            padding: 20px;
            border-radius: 0 6px 6px 0;
            margin: 20px 0;
        }

        /* Chart containers */
        .chart-container {
            background: rgba(13, 17, 23, 0.5);
            padding: 20px;
            border-radius: 6px;
            border: 1px solid rgba(48, 54, 61, 0.5);
        }

        /* Badge */
        .badge {
            display: inline-block;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.7rem;
            font-weight: 600;
            letter-spacing: 0.5px;
            text-transform: uppercase;
            background: rgba(88, 166, 255, 0.15);
            color: #58a6ff;
            border: 1px solid rgba(88, 166, 255, 0.3);
        }

        /* Section cards */
        .section-card {
            background: rgba(22, 27, 34, 0.6);
            backdrop-filter: blur(8px);
            border: 1px solid rgba(48, 54, 61, 0.6);
            border-radius: 8px;
            padding: 32px;
            transition: all 0.3s ease;
        }
        
        .section-card:hover {
            border-color: rgba(88, 166, 255, 0.3);
        }

        /* Number badges */
        .number-badge {
            width: 40px;
            height: 40px;
            border-radius: 8px;
            background: rgba(88, 166, 255, 0.15);
            border: 1px solid rgba(88, 166, 255, 0.3);
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            color: #58a6ff;
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 10px;
            height: 10px;
        }
        
        ::-webkit-scrollbar-track {
            background: #0d1117;
        }
        
        ::-webkit-scrollbar-thumb {
            background: rgba(88, 166, 255, 0.3);
            border-radius: 5px;
        }
        
        ::-webkit-scrollbar-thumb:hover {
            background: rgba(88, 166, 255, 0.5);
        }

        /* Status indicator */
        .status-indicator {
            display: inline-block;
            width: 6px;
            height: 6px;
            border-radius: 50%;
            background: #58a6ff;
            margin-right: 6px;
            animation: pulse 2s ease-in-out infinite;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.4; }
        }

        /* Subtle highlights */
        h1, h2, h3 {
            color: #e6edf3;
        }
        
        strong {
            color: #e6edf3;
            font-weight: 600;
        }
    </style>
</head>
<body class="antialiased">
    
    <!-- Animated Background -->
    <div class="animated-bg"></div>

    <!-- Header/Navigation -->
    <header class="navbar fixed top-0 left-0 right-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <div>
                <h1 class="text-lg font-semibold" style="color: #e6edf3;">Gradient Descent Optimizers</h1>
                <span class="badge mt-1">Deep Learning</span>
            </div>
            <div class="hidden md:flex space-x-6">
                <a href="#intro" class="nav-link">Setup</a>
                <a href="#sgd" class="nav-link">SGD</a>
                <a href="#decay" class="nav-link">Decay</a>
                <a href="#momentum" class="nav-link">Momentum</a>
                <a href="#adagrad" class="nav-link">AdaGrad</a>
                <a href="#rmsprop" class="nav-link">RMSProp</a>
                <a href="#adam" class="nav-link">Adam</a>
            </div>
        </nav>
    </header>

    <main class="container mx-auto px-6 pt-28 pb-12">

        <!-- Hero Section -->
        <section class="text-center my-16 fade-in-section">
            <div class="mb-4">
                <span class="text-xs font-medium tracking-wider uppercase accent-muted">Technical Deep Dive</span>
            </div>
            <h1 class="text-5xl md:text-6xl font-bold leading-tight mb-4">
                Understanding Gradient Descent Optimizers
            </h1>
            <h2 class="text-2xl md:text-3xl font-light mt-2 mb-6 accent-muted">From SGD to Adam</h2>
            <p class="mt-6 max-w-3xl mx-auto text-base accent-muted leading-relaxed">
                A comprehensive analysis of optimization algorithms in neural network training. 
                This guide covers the mathematical foundations, implementation details, and practical considerations 
                for selecting and tuning optimizers in production environments.
            </p>
            
            <!-- Neural Network Animation -->
            <div class="mt-12 max-w-5xl mx-auto">
                <canvas id="neuralNetCanvas"></canvas>
                <p class="mt-3 text-xs accent-muted flex items-center justify-center gap-2">
                    <span class="status-indicator"></span>
                    <span>Live gradient descent simulation</span>
                </p>
            </div>
        </section>

        <!-- Introduction Section -->
        <section id="intro" class="my-16 py-8 fade-in-section">
            <div class="section-card">
                <h2 class="text-3xl font-bold mb-6">Introduction & Experimental Setup</h2>
                <p class="text-base accent-muted mb-6 leading-relaxed">
                    Optimization algorithms are fundamental to training neural networks effectively. While vanilla gradient descent 
                    provides the theoretical foundation, modern optimizers incorporate adaptive learning rates, momentum, and 
                    variance reduction techniques to improve convergence properties. This guide provides both theoretical analysis 
                    and empirical comparisons of major optimization algorithms.
                </p>
                
                <h3 class="text-2xl font-semibold mt-10 mb-5">Synthetic Dataset Generation</h3>
                <p class="text-base accent-muted mb-5 leading-relaxed">
                    We construct a simple linear regression problem: $y = 3x + 7 + \epsilon$ where $\epsilon \sim \mathcal{N}(0, 0.5^2)$. 
                    The objective is to recover parameters $w$ and $b$ by minimizing the Mean Squared Error loss function:
                </p>
                
                <div class="bg-card p-5 rounded-lg my-6 border border-gray-700">
                    <p class="text-center font-mono text-sm">
                        $$ \mathcal{L}(w, b) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2 \quad \text{where} \quad \hat{y} = Xw + b $$
                    </p>
                </div>
                
                <p class="text-base accent-muted mb-5 leading-relaxed">
                    The gradients are computed analytically using the chain rule:
                </p>
                
                <div class="grid md:grid-cols-2 gap-5 mb-6">
                    <div class="bg-card p-5 rounded-lg border border-gray-700">
                        <p class="text-center font-mono text-sm">
                            $$ \frac{\partial \mathcal{L}}{\partial w} = \frac{2}{m} X^T (\hat{y} - y) $$
                        </p>
                    </div>
                    <div class="bg-card p-5 rounded-lg border border-gray-700">
                        <p class="text-center font-mono text-sm">
                            $$ \frac{\partial \mathcal{L}}{\partial b} = \frac{2}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i) $$
                        </p>
                    </div>
                </div>

                <details class="mb-6">
                    <summary>View implementation details</summary>
                    <div class="mt-4 relative">
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# Set random seed for reproducibility
np.random.seed(42)

# Generate input features from standard normal distribution
X = np.random.randn(100, 1)

# Generate targets with additive Gaussian noise
y = 3 * X + 7 + np.random.randn(100, 1) * 0.5

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.5, s=40, c='#58a6ff', edgecolors='none')
plt.plot(X, 3*X + 7, 'r--', linewidth=2, alpha=0.7, label='True: y = 3x + 7')
plt.xlabel('X', fontsize=11)
plt.ylabel('y', fontsize=11)
plt.title('Synthetic Linear Regression Dataset', fontsize=12, fontweight='600')
plt.legend(frameon=False)
plt.grid(alpha=0.2, linestyle='--')
plt.tight_layout()
plt.show()
</code><button class="copy-btn">Copy</button></pre>
                    </div>
                </details>
                
                <!-- Dataset Visualization -->
                <div class="mt-6 chart-container">
                    <h4 class="text-lg font-semibold mb-4">Dataset Visualization</h4>
                    <canvas id="datasetChart"></canvas>
                </div>
            </div>
        </section>

        <!-- Core Content Sections -->
        <div class="space-y-16">
            
            <!-- Vanilla SGD -->
            <section id="sgd" class="section-card fade-in-section">
                <div class="flex items-center gap-4 mb-6">
                    <div class="number-badge">1</div>
                    <h2 class="text-3xl font-bold">Stochastic Gradient Descent</h2>
                </div>
                <p class="text-base accent-muted mb-5 leading-relaxed">
                    The foundational first-order optimization algorithm. Updates parameters in the opposite direction 
                    of the gradient, scaled by a fixed learning rate $\alpha$.
                </p>

                <div class="explanation-box">
                    <h3 class="text-xl font-semibold mb-4">Algorithm Analysis</h3>
                    <p class="text-base accent-muted mb-5 leading-relaxed">
                        SGD performs parameter updates according to: $\theta_{t+1} = \theta_t - \alpha \nabla_\theta \mathcal{L}(\theta_t)$. 
                        While simple and computationally efficient, the fixed learning rate presents challenges in non-convex optimization landscapes. 
                        The algorithm exhibits oscillatory behavior in regions with high curvature and slow convergence on plateau regions.
                    </p>
                    <div class="grid md:grid-cols-2 gap-5">
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-blue">Advantages</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Minimal computational overhead per iteration</li>
                                <li>Low memory footprint (no auxiliary variables)</li>
                                <li>Well-suited for convex optimization</li>
                                <li>Interpretable hyperparameter (learning rate only)</li>
                            </ul>
                        </div>
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-muted">Limitations</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>High sensitivity to learning rate selection</li>
                                <li>Prone to oscillation in steep valleys</li>
                                <li>Slow convergence on flat surfaces</li>
                                <li>Difficulty escaping saddle points</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="grid md:grid-cols-2 gap-6 mt-6">
                    <div class="chart-container">
                        <h4 class="text-lg font-semibold mb-4">Loss Convergence</h4>
                        <canvas id="sgdChart"></canvas>
                    </div>
                    <div class="relative">
<pre><code class="language-python">def train_sgd(X, y, lr=0.01, epochs=100):
    """
    Vanilla SGD implementation for linear regression.
    
    Parameters:
        X: Feature matrix (m x n)
        y: Target vector (m x 1)
        lr: Learning rate (step size)
        epochs: Number of training iterations
        
    Returns:
        w, b: Learned parameters
        losses: Training loss history
    """
    # Initialize parameters
    w = np.random.randn(1, 1)
    b = np.random.randn(1)
    losses = []
    m = len(y)
    
    for epoch in range(epochs):
        # Forward pass
        y_pred = X @ w + b
        
        # Compute loss
        loss = np.mean((y_pred - y) ** 2)
        losses.append(loss)
        
        # Compute gradients
        dw = (2 / m) * X.T @ (y_pred - y)
        db = (2 / m) * np.sum(y_pred - y)
        
        # Parameter update
        w -= lr * dw
        b -= lr * db
    
    return w, b, losses
</code><button class="copy-btn">Copy</button></pre>
                    </div>
                </div>
            </section>

            <!-- SGD with Decay -->
            <section id="decay" class="section-card fade-in-section">
                <div class="flex items-center gap-4 mb-6">
                    <div class="number-badge">2</div>
                    <h2 class="text-3xl font-bold">Learning Rate Scheduling</h2>
                </div>
                <p class="text-base accent-muted mb-5 leading-relaxed">
                    Dynamic learning rate adjustment improves convergence by taking larger steps initially and 
                    smaller steps near optima. Inverse time decay provides a smooth, monotonic schedule.
                </p>

                <div class="explanation-box">
                    <h3 class="text-xl font-semibold mb-4">Algorithm Analysis</h3>
                    <p class="text-base accent-muted mb-5 leading-relaxed">
                        The learning rate decays according to: $\alpha_t = \frac{\alpha_0}{1 + \gamma t}$ where $\gamma$ is the decay rate. 
                        This approach balances exploration in early epochs with exploitation in later stages. The decay schedule must be 
                        calibrated to prevent premature convergence or excessive training time.
                    </p>
                    <div class="bg-card p-4 rounded-lg border border-gray-700 mb-5">
                        <p class="text-center font-mono text-sm">
                            $$ \alpha_t = \frac{\alpha_0}{1 + \gamma \cdot t} $$
                        </p>
                    </div>
                    <div class="grid md:grid-cols-2 gap-5">
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-blue">Advantages</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Reduced oscillation near convergence</li>
                                <li>Often achieves lower final loss values</li>
                                <li>Simple to implement</li>
                                <li>Theoretically grounded convergence properties</li>
                            </ul>
                        </div>
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-muted">Limitations</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Requires tuning of decay hyperparameter</li>
                                <li>Aggressive decay can stall training prematurely</li>
                                <li>Not adaptive to local geometry</li>
                                <li>May require schedule warm-up</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="grid md:grid-cols-2 gap-6 mt-6">
                    <div class="chart-container">
                        <h4 class="text-lg font-semibold mb-4">Decay Schedule Impact</h4>
                        <canvas id="decayChart"></canvas>
                    </div>
                    <div class="relative">
<pre><code class="language-python">def train_sgd_decay(X, y, lr_initial=0.1, 
                    decay=0.05, epochs=100):
    """
    SGD with inverse time decay schedule.
    
    The effective learning rate decreases over time:
    lr(t) = lr_0 / (1 + decay * t)
    
    Parameters:
        lr_initial: Initial learning rate
        decay: Decay rate coefficient
    """
    w, b = np.random.randn(1,1), np.random.randn(1)
    losses, m = [], len(y)
    
    for epoch in range(epochs):
        # Apply decay schedule
        lr = lr_initial / (1 + decay * epoch)
        
        # Forward pass
        y_pred = X @ w + b
        losses.append(np.mean((y_pred - y)**2))
        
        # Compute gradients
        dw = (2/m) * X.T @ (y_pred - y)
        db = (2/m) * np.sum(y_pred - y)
        
        # Update with decayed learning rate
        w -= lr * dw
        b -= lr * db
    
    return w, b, losses
</code><button class="copy-btn">Copy</button></pre>
                    </div>
                </div>
            </section>

            <!-- Momentum -->
            <section id="momentum" class="section-card fade-in-section">
                <div class="flex items-center gap-4 mb-6">
                    <div class="number-badge">3</div>
                    <h2 class="text-3xl font-bold">Momentum-Based Optimization</h2>
                </div>
                <p class="text-base accent-muted mb-5 leading-relaxed">
                    Incorporates exponentially weighted moving averages of past gradients to accelerate learning 
                    in consistent directions and dampen oscillations perpendicular to the descent path.
                </p>

                <div class="explanation-box">
                    <h3 class="text-xl font-semibold mb-4">Algorithm Analysis</h3>
                    <p class="text-base accent-muted mb-5 leading-relaxed">
                        Momentum maintains a velocity vector $v_t$ that accumulates gradients: $v_t = \beta v_{t-1} + (1-\beta)\nabla_\theta \mathcal{L}$, 
                        followed by the update: $\theta_{t+1} = \theta_t - \alpha v_t$. The momentum coefficient $\beta$ (typically 0.9) 
                        controls the exponential decay of past gradients. This mechanism enables the optimizer to build inertia in 
                        directions of persistent gradient alignment while smoothing out high-frequency oscillations.
                    </p>
                    <div class="bg-card p-4 rounded-lg border border-gray-700 mb-5">
                        <p class="text-center font-mono text-sm">
                            $$ v_t = \beta v_{t-1} + (1-\beta) \nabla_\theta \mathcal{L}, \quad \theta_{t+1} = \theta_t - \alpha v_t $$
                        </p>
                    </div>
                    <div class="grid md:grid-cols-2 gap-5">
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-blue">Advantages</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Accelerated convergence in ravine-like landscapes</li>
                                <li>Effective dampening of gradient noise</li>
                                <li>Improved escape from local minima</li>
                                <li>Reduced sensitivity to learning rate</li>
                            </ul>
                        </div>
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-muted">Limitations</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Potential overshooting with high momentum</li>
                                <li>Additional hyperparameter to tune</li>
                                <li>May bypass sharp but desirable minima</li>
                                <li>Requires careful initialization</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="grid md:grid-cols-2 gap-6 mt-6">
                    <div class="chart-container">
                        <h4 class="text-lg font-semibold mb-4">Momentum Coefficient Analysis</h4>
                        <canvas id="momentumChart"></canvas>
                    </div>
                    <div class="relative">
<pre><code class="language-python">def train_sgd_momentum(X, y, lr=0.01, 
                       momentum=0.9, epochs=100):
    """
    SGD with momentum acceleration.
    
    Maintains exponentially weighted moving average
    of gradients to smooth updates and build inertia.
    
    Parameters:
        momentum: Decay factor for velocity (β)
                 Typical values: 0.9, 0.95, 0.99"""
    w, b = np.random.randn(1,1), np.random.randn(1)
    losses, m = [], len(y)
    
    # Initialize velocity vectors
    v_w, v_b = np.zeros_like(w), np.zeros_like(b)

    for epoch in range(epochs):
        # Forward pass
        y_pred = X @ w + b
        losses.append(np.mean((y_pred - y)**2))
        
        # Compute gradients
        dw = (2/m) * X.T @ (y_pred - y)
        db = (2/m) * np.sum(y_pred - y)

        # Update velocity (exponential moving average)
        v_w = momentum * v_w + (1-momentum) * dw
        v_b = momentum * v_b + (1-momentum) * db
        
        # Parameter update using velocity
        w -= lr * v_w
        b -= lr * v_b

    return w, b, losses
</code><button class="copy-btn">Copy</button></pre>
                    </div>
                </div>
            </section>
            
            <!-- AdaGrad -->
            <section id="adagrad" class="section-card fade-in-section">
                <div class="flex items-center gap-4 mb-6">
                    <div class="number-badge">4</div>
                    <h2 class="text-3xl font-bold">AdaGrad: Adaptive Gradient Algorithm</h2>
                </div>
                <p class="text-base accent-muted mb-5 leading-relaxed">
                    Adapts learning rates on a per-parameter basis using accumulated squared gradients. 
                    Particularly effective for sparse gradient scenarios encountered in natural language processing.
                </p>

                <div class="explanation-box">
                    <h3 class="text-xl font-semibold mb-4">Algorithm Analysis</h3>
                    <p class="text-base accent-muted mb-5 leading-relaxed">
                        AdaGrad maintains an accumulator $G_t = G_{t-1} + g_t^2$ for each parameter, where $g_t = \nabla_\theta \mathcal{L}$. 
                        Updates are computed as: $\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}} g_t$. 
                        Parameters with historically large gradients receive proportionally smaller updates, while infrequently updated 
                        parameters benefit from larger effective learning rates. The monotonic accumulation of $G_t$ leads to 
                        continual learning rate decay, which can be problematic in non-convex settings.
                    </p>
                    <div class="bg-card p-4 rounded-lg border border-gray-700 mb-5">
                        <p class="text-center font-mono text-sm">
                            $$ G_t = G_{t-1} + g_t^2, \quad \theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}} g_t $$
                        </p>
                    </div>
                    <div class="grid md:grid-cols-2 gap-5">
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-blue">Advantages</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Adaptive per-parameter learning rates</li>
                                <li>Effective for sparse feature spaces</li>
                                <li>Eliminates need for manual lr tuning per parameter</li>
                                <li>Well-suited for online learning scenarios</li>
                            </ul>
                        </div>
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-muted">Limitations</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Unbounded accumulator growth</li>
                                <li>Learning rate decay to near-zero</li>
                                <li>Premature convergence in deep networks</li>
                                <li>No momentum component</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="grid md:grid-cols-2 gap-6 mt-6">
                    <div class="chart-container">
                        <h4 class="text-lg font-semibold mb-4">Comparative Analysis</h4>
                        <canvas id="adagradChart"></canvas>
                    </div>
                    <div class="relative">
<pre><code class="language-python">def train_adagrad(X, y, lr=0.1, 
                  epochs=100, epsilon=1e-8):
    """
    AdaGrad optimizer implementation.
    
    Adapts learning rates based on historical
    squared gradient magnitudes. Accumulator grows
    monotonically, leading to decay.
    
    Parameters:
        epsilon: Numerical stability constant
    """
    w, b = np.random.randn(1,1), np.random.randn(1)
    losses, m = [], len(y)
    
    # Initialize gradient accumulators
    g_w, g_b = np.zeros_like(w), np.zeros_like(b)
    
    for epoch in range(epochs):
        # Forward pass
        y_pred = X @ w + b
        losses.append(np.mean((y_pred - y)**2))
        
        # Compute gradients
        dw = (2/m) * X.T @ (y_pred - y)
        db = (2/m) * np.sum(y_pred - y)
        
        # Accumulate squared gradients
        g_w += dw**2
        g_b += db**2
        
        # Adaptive parameter update
        w -= (lr / (np.sqrt(g_w + epsilon))) * dw
        b -= (lr / (np.sqrt(g_b + epsilon))) * db
        
    return w, b, losses
</code><button class="copy-btn">Copy</button></pre>
                    </div>
                </div>
            </section>
            
            <!-- RMSProp -->
            <section id="rmsprop" class="section-card fade-in-section">
                <div class="flex items-center gap-4 mb-6">
                    <div class="number-badge">5</div>
                    <h2 class="text-3xl font-bold">RMSProp: Root Mean Square Propagation</h2>
                </div>
                <p class="text-base accent-muted mb-5 leading-relaxed">
                    Addresses AdaGrad's aggressive learning rate decay by using exponentially weighted moving averages 
                    of squared gradients, enabling sustained learning in non-stationary environments.
                </p>

                <div class="explanation-box">
                    <h3 class="text-xl font-semibold mb-4">Algorithm Analysis</h3>
                    <p class="text-base accent-muted mb-5 leading-relaxed">
                        RMSProp replaces AdaGrad's cumulative sum with an exponential moving average: 
                        $E[g^2]_t = \beta E[g^2]_{t-1} + (1-\beta)g_t^2$, followed by the update: 
                        $\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} g_t$. 
                        The decay parameter $\beta$ (typically 0.9 or 0.99) implements a "forgetting factor" that 
                        allows the optimizer to respond to recent gradient information while maintaining stability. 
                        This modification prevents the learning rate from vanishing and is particularly effective 
                        for recurrent architectures with non-stationary dynamics.
                    </p>
                    <div class="bg-card p-4 rounded-lg border border-gray-700 mb-5">
                        <p class="text-center font-mono text-sm">
                            $$ E[g^2]_t = \beta E[g^2]_{t-1} + (1-\beta)g_t^2, \quad \theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} g_t $$
                        </p>
                    </div>
                    <div class="grid md:grid-cols-2 gap-5">
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-blue">Advantages</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Sustained learning rate throughout training</li>
                                <li>Adaptive to non-stationary objectives</li>
                                <li>Effective for recurrent neural networks</li>
                                <li>Balances responsiveness and stability</li>
                            </ul>
                        </div>
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-muted">Limitations</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Requires tuning of decay parameter</li>
                                <li>No bias correction mechanism</li>
                                <li>Lacks momentum component</li>
                                <li>Sensitive to initialization scale</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="grid md:grid-cols-2 gap-6 mt-6">
                    <div class="chart-container">
                        <h4 class="text-lg font-semibold mb-4">Adaptive Methods Comparison</h4>
                        <canvas id="rmspropChart"></canvas>
                    </div>
                    <div class="relative">
<pre><code class="language-python">def train_rmsprop(X, y, lr=0.01, beta=0.9, 
                  epochs=100, epsilon=1e-8):
    """
    RMSProp optimizer implementation.
    
    Uses exponentially decaying average of squared
    gradients to prevent learning rate collapse.
    
    Parameters:
        beta: Decay rate for moving average
              Common values: 0.9, 0.99
    """
    w, b = np.random.randn(1,1), np.random.randn(1)
    losses, m = [], len(y)
    
    # Initialize moving average accumulators
    g_w, g_b = np.zeros_like(w), np.zeros_like(b)
    
    for epoch in range(epochs):
        # Forward pass
        y_pred = X @ w + b
        losses.append(np.mean((y_pred - y)**2))
        
        # Compute gradients
        dw = (2/m) * X.T @ (y_pred - y)
        db = (2/m) * np.sum(y_pred - y)
        
        # Update moving average of squared gradients
        g_w = beta * g_w + (1-beta) * (dw**2)
        g_b = beta * g_b + (1-beta) * (db**2)
        
        # Adaptive update using RMS scaling
        w -= (lr / (np.sqrt(g_w + epsilon))) * dw
        b -= (lr / (np.sqrt(g_b + epsilon))) * db
        
    return w, b, losses
</code><button class="copy-btn">Copy</button></pre>
                    </div>
                </div>
            </section>
            
            <!-- Adam -->
            <section id="adam" class="section-card fade-in-section">
                <div class="flex items-center gap-4 mb-6">
                    <div class="number-badge">6</div>
                    <h2 class="text-3xl font-bold">Adam: Adaptive Moment Estimation</h2>
                </div>
                <p class="text-base accent-muted mb-5 leading-relaxed">
                    Combines momentum with adaptive learning rates and incorporates bias correction for moment estimates. 
                    Currently the de facto standard optimizer for deep learning applications.
                </p>

                <div class="explanation-box">
                    <h3 class="text-xl font-semibold mb-4">Algorithm Analysis</h3>
                    <p class="text-base accent-muted mb-5 leading-relaxed">
                        Adam maintains both first moment (mean) $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$ and second moment (uncentered variance) 
                        $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$ estimates. These are bias-corrected via: 
                        $\hat{m}_t = m_t/(1-\beta_1^t)$ and $\hat{v}_t = v_t/(1-\beta_2^t)$, followed by the update: 
                        $\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$. 
                        The bias correction is critical in early iterations when moment estimates are initialized at zero. 
                        Default hyperparameters ($\beta_1=0.9$, $\beta_2=0.999$, $\alpha=0.001$) are robust across diverse problem domains.
                    </p>
                    <div class="bg-card p-4 rounded-lg border border-gray-700 mb-5">
                        <p class="text-center font-mono text-xs">
                            $$ m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t, \quad v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 $$
                            $$ \hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}, \quad \theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t $$
                        </p>
                    </div>
                    <div class="grid md:grid-cols-2 gap-5">
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-blue">Advantages</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Robust default hyperparameters</li>
                                <li>Combines benefits of momentum and adaptation</li>
                                <li>Bias correction for accurate moment estimation</li>
                                <li>Efficient convergence on most architectures</li>
                                <li>Minimal tuning required in practice</li>
                            </ul>
                        </div>
                        <div>
                            <h4 class="text-base font-semibold mb-3 accent-muted">Limitations</h4>
                            <ul class="list-disc pl-5 space-y-1 text-sm accent-muted">
                                <li>Higher memory overhead (two moment buffers)</li>
                                <li>May converge to sharp minima</li>
                                <li>Potential generalization gap vs SGD</li>
                                <li>More hyperparameters than simpler methods</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="grid md:grid-cols-2 gap-6 mt-6">
                    <div class="chart-container">
                        <h4 class="text-lg font-semibold mb-4">Comprehensive Optimizer Comparison</h4>
                        <canvas id="adamChart"></canvas>
                    </div>
                    <div class="relative">
<pre><code class="language-python">def train_adam(X, y, lr=0.001, epochs=100,
               beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    Adam optimizer implementation.
    
    Combines first and second moment estimates with
    bias correction for improved early-stage learning.
    
    Parameters:
        beta1: Exponential decay for first moment
        beta2: Exponential decay for second moment
        
    Note: Default parameters work well in most cases
    """
    w, b = np.random.randn(1,1), np.random.randn(1)
    losses, m = [], len(y)
    
    # Initialize moment estimates
    m_w, v_w = np.zeros_like(w), np.zeros_like(w)
    m_b, v_b = np.zeros_like(b), np.zeros_like(b)
    
    for t in range(1, epochs + 1):
        # Forward pass
        y_pred = X @ w + b
        losses.append(np.mean((y_pred - y)**2))
        
        # Compute gradients
        dw = (2/m) * X.T @ (y_pred - y)
        db = (2/m) * np.sum(y_pred - y)
        
        # Update biased first moment estimate
        m_w = beta1 * m_w + (1 - beta1) * dw
        m_b = beta1 * m_b + (1 - beta1) * db
        
        # Update biased second moment estimate
        v_w = beta2 * v_w + (1 - beta2) * (dw ** 2)
        v_b = beta2 * v_b + (1 - beta2) * (db ** 2)
        
        # Compute bias-corrected moment estimates
        m_w_hat = m_w / (1 - beta1 ** t)
        v_w_hat = v_w / (1 - beta2 ** t)
        m_b_hat = m_b / (1 - beta1 ** t)
        v_b_hat = v_b / (1 - beta2 ** t)
        
        # Adam update rule
        w -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
        b -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)
        
    return w, b, losses
</code><button class="copy-btn">Copy</button></pre>
                    </div>
                </div>
            </section>
        </div>

        <!-- Key Takeaways -->
        <section class="my-16 py-8 fade-in-section">
            <div class="text-center mb-10">
                <h2 class="text-4xl font-bold mb-3">Practical Guidelines</h2>
                <p class="text-base accent-muted">Selection criteria for optimization algorithms</p>
            </div>
            
            <div class="custom-table">
                <!-- Header -->
                <div class="table-cell table-header">Algorithm</div>
                <div class="table-cell table-header">Key Strengths</div>
                <div class="table-cell table-header">Primary Limitations</div>
                <div class="table-cell table-header">Recommended Use Cases</div>
                
                <!-- SGD -->
                <div class="table-cell" data-label="Algorithm">
                    <div class="font-semibold accent-blue">SGD</div>
                </div>
                <div class="table-cell" data-label="Key Strengths">
                    <span class="text-sm">Minimal overhead, interpretable, good generalization</span>
                </div>
                <div class="table-cell" data-label="Primary Limitations">
                    <span class="text-sm">Slow convergence, sensitive to learning rate</span>
                </div>
                <div class="table-cell" data-label="Recommended Use Cases">
                    <span class="text-sm">Convex problems, final fine-tuning</span>
                </div>
                
                <!-- SGD + Decay -->
                <div class="table-cell" data-label="Algorithm">
                    <div class="font-semibold accent-blue">SGD + Decay</div>
                </div>
                <div class="table-cell" data-label="Key Strengths">
                    <span class="text-sm">Reduced oscillation, smoother convergence</span>
                </div>
                <div class="table-cell" data-label="Primary Limitations">
                    <span class="text-sm">Schedule tuning required, premature decay risk</span>
                </div>
                <div class="table-cell" data-label="Recommended Use Cases">
                    <span class="text-sm">Extended training runs, known schedule</span>
                </div>
                
                <!-- Momentum -->
                <div class="table-cell" data-label="Algorithm">
                    <div class="font-semibold accent-blue">Momentum</div>
                </div>
                <div class="table-cell" data-label="Key Strengths">
                    <span class="text-sm">Faster convergence, noise dampening</span>
                </div>
                <div class="table-cell" data-label="Primary Limitations">
                    <span class="text-sm">Potential overshooting, extra hyperparameter</span>
                </div>
                <div class="table-cell" data-label="Recommended Use Cases">
                    <span class="text-sm">Deep networks, noisy gradients</span>
                </div>
                
                <!-- AdaGrad -->
                <div class="table-cell" data-label="Algorithm">
                    <div class="font-semibold accent-blue">AdaGrad</div>
                </div>
                <div class="table-cell" data-label="Key Strengths">
                    <span class="text-sm">Per-parameter adaptation, sparse data handling</span>
                </div>
                <div class="table-cell" data-label="Primary Limitations">
                    <span class="text-sm">Learning rate decay to zero</span>
                </div>
                <div class="table-cell" data-label="Recommended Use Cases">
                    <span class="text-sm">NLP, sparse features, online learning</span>
                </div>
                
                <!-- RMSProp -->
                <div class="table-cell" data-label="Algorithm">
                    <div class="font-semibold accent-blue">RMSProp</div>
                </div>
                <div class="table-cell" data-label="Key Strengths">
                    <span class="text-sm">Non-monotonic decay, non-stationary objectives</span>
                </div>
                <div class="table-cell" data-label="Primary Limitations">
                    <span class="text-sm">No bias correction, decay parameter sensitivity</span>
                </div>
                <div class="table-cell" data-label="Recommended Use Cases">
                    <span class="text-sm">RNNs, online learning, streaming data</span>
                </div>
                
                <!-- Adam -->
                <div class="table-cell" data-label="Algorithm">
                    <div class="font-semibold accent-blue">Adam</div>
                </div>
                <div class="table-cell" data-label="Key Strengths">
                    <span class="text-sm">Robust defaults, fast convergence, minimal tuning</span>
                </div>
                <div class="table-cell" data-label="Primary Limitations">
                    <span class="text-sm">Memory overhead, possible generalization gap</span>
                </div>
                <div class="table-cell" data-label="Recommended Use Cases">
                    <span class="text-sm">General-purpose, CNNs, Transformers</span>
                </div>
            </div>
            
            <div class="mt-10 bg-card rounded-lg p-6 border border-gray-700">
                <h3 class="text-lg font-semibold mb-4">Implementation Recommendations</h3>
                <div class="grid md:grid-cols-3 gap-5 text-sm">
                    <div class="p-4 bg-gray-900 bg-opacity-40 rounded-lg">
                        <p class="font-semibold mb-2 accent-blue">Default Choice</p>
                        <p class="accent-muted">Start with Adam (lr=1e-3). Provides strong baseline performance across most architectures.</p>
                    </div>
                    <div class="p-4 bg-gray-900 bg-opacity-40 rounded-lg">
                        <p class="font-semibold mb-2 accent-blue">Production Training</p>
                        <p class="accent-muted">Consider SGD with momentum for final training when generalization is critical.</p>
                    </div>
                    <div class="p-4 bg-gray-900 bg-opacity-40 rounded-lg">
                        <p class="font-semibold mb-2 accent-blue">Stability</p>
                        <p class="accent-muted">Always implement gradient clipping to prevent numerical instability.</p>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <footer class="text-center py-6 border-t border-gray-800">
        <p class="text-xs accent-muted">Technical documentation on gradient-based optimization algorithms</p>
        <p class="text-xs accent-muted mt-1">Implementation reference for deep learning practitioners</p>
    </footer>

<script>
document.addEventListener('DOMContentLoaded', () => {

    // --- SYNTHETIC DATA ---
    const syntheticData = [
        { x: 0.4967141530112327, y: 7.782457088008492 },
        { x: -0.13826430117118466, y: 6.374884435103766 },
        { x: 0.6476885381006925, y: 8.771708356038694 },
        { x: 1.5230298564080254, y: 11.167950934613266 },
        { x: -0.23415337472333597, y: 6.2168970199969875 },
        { x: -0.23413695694918055, y: 6.4996145575597275 },
        { x: 1.5792128155073915, y: 12.68073139712744 },
        { x: 0.7674347291529088, y: 9.389593093874646 },
        { x: -0.4694743859349521, y: 5.720352037556526 },
        { x: 0.5425600435859647, y: 8.590457172874812 },
        { x: -0.46341769281246226, y: 4.650361313913092 },
        { x: -0.46572975357025687, y: 5.589553801564621 },
        { x: 0.24196227156603412, y: 7.756001919668615 },
        { x: -1.913280244657798, y: 2.49178032226925 },
        { x: -1.7249178325130328, y: 1.72906602007034 },
        { x: -0.5622875292409727, y: 5.463911083443889 },
        { x: -1.0128311203344238, y: 3.9441507541441068 },
        { x: 0.3142473325952739, y: 7.358402978976056 },
        { x: -0.9080240755212109, y: 4.8473391806938775 },
        { x: -1.4123037013352915, y: 3.139055412337513 },
        { x: 1.465648768921554, y: 11.792462280286184 },
        { x: -0.22577630048653566, y: 5.867977371143024 },
        { x: 0.06752820468792384, y: 7.903981769531821 },
        { x: -1.4247481862134568, y: 2.024829909963489 },
        { x: -0.5443827245251827, y: 5.660280373324587 },
        { x: 0.11092258970986608, y: 8.427995582034587 },
        { x: -1.1509935774223028, y: 3.0517511051677473 },
        { x: 0.37569801834567196, y: 7.8439451902356305 },
        { x: -0.600638689918805, y: 5.247909612787406 },
        { x: -0.2916937497932768, y: 5.87318092356207 },
        { x: -0.6017066122293969, y: 4.419548447778744 },
        { x: 1.8522781845089378, y: 12.591116040929826 },
        { x: -0.013497224737933921, y: 6.4283564689231465 },
        { x: -1.0577109289559004, y: 4.06366342844989 },
        { x: 0.822544912103189, y: 9.007922619192666 },
        { x: -1.2208436499710222, y: 4.112436252595703 },
        { x: 0.2088635950047554, y: 7.234964138846148 },
        { x: -1.9596701238797756, y: 0.9599588702578357 },
        { x: -1.3281860488984305, y: 3.4222004619895436 },
        { x: 0.19686123586912352, y: 6.975151549390393 },
        { x: 0.7384665799954104, y: 9.329129707288297 },
        { x: 0.1713682811899705, y: 8.167676220711126 },
        { x: -0.11564828238824053, y: 5.849313535554665 },
        { x: -0.3011036955892888, y: 6.189005842498286 },
        { x: -1.4785219903674274, y: 2.69437542602193 },
        { x: -0.7198442083947086, y: 5.23137881070453 },
        { x: -0.4606387709597875, y: 4.999608331681596 },
        { x: 1.0571222262189157, y: 9.51113837211461 },
        { x: 0.3436182895684614, y: 8.291825651513834 },
        { x: -1.763040155362734, y: 1.8593718705283917 },
        { x: 0.324083969394795, y: 8.097498333357322 },
        { x: -0.38508228041631654, y: 6.017977263499538 },
        { x: -0.6769220003059587, y: 4.629221638292878 },
        { x: 0.6116762888408679, y: 8.951155715103106 },
        { x: 1.030999522495951, y: 10.239534804137195 },
        { x: 0.9312801191161986, y: 9.436664648335412 },
        { x: -0.8392175232226385, y: 5.415234685904463 },
        { x: -0.3092123758512146, y: 6.30927933290225 },{ x: 0.33126343140356396, y: 7.398138545609368 },
        { x: 0.9755451271223592, y: 10.254912185683992 },
        { x: -0.47917423784528995, y: 5.0751364513504695 },
        { x: -0.18565897666381712, y: 6.836565371879774 },
        { x: -1.1063349740060282, y: 4.260292867485617 },
        { x: -1.1962066240806708, y: 3.001038968582132 },
        { x: 0.812525822394198, y: 9.919265531804754 },
        { x: 1.356240028570823, y: 11.275110549180718 },
        { x: -0.07201012158033385, y: 7.194999715256243 },
        { x: 1.0035328978920242, y: 10.958995185003047 },
        { x: 0.36163602504763415, y: 7.962214017141467 },
        { x: -0.6451197546051243, y: 4.687772654005882 },
        { x: 0.36139560550841393, y: 7.63942960171248 },
        { x: 1.5380365664659692, y: 11.20620455691519 },
        { x: -0.03582603910995154, y: 6.853971027963094 },
        { x: 1.5646436558140062, y: 11.86450695485034 },
        { x: -2.6197451040897444, y: -0.7208899126042232 },
        { x: 0.8219025043752238, y: 9.879299137643685 },
        { x: 0.08704706823817122, y: 7.267642150653468 },
        { x: -0.29900735046586746, y: 6.829744987181056 },
        { x: 0.0917607765355023, y: 7.1429539129875295 },
        { x: -1.9875689146008928, y: 2.3973778394921306 },
        { x: -0.21967188783751193, y: 6.653818010369967 },
        { x: 0.3571125715117464, y: 7.642758936327098 },
        { x: 1.477894044741516, y: 10.89823588519399 },
        { x: -0.5182702182736474, y: 5.68642555280065 },
        { x: -0.8084936028931876, y: 4.462787798657512 },
        { x: -0.5017570435845365, y: 5.851729116292437 },
        { x: 0.9154021177020741, y: 9.982825165392995 },
        { x: 0.32875110965968446, y: 7.949838872650617 },
        { x: -0.5297602037670388, y: 4.987322529664681 },
        { x: 0.5132674331133561, y: 7.782378686997135 },
        { x: 0.09707754934804039, y: 7.067975172010611 },
        { x: 0.9686449905328892, y: 10.334134368760404 },
        { x: -0.7020530938773524, y: 5.000887590433045 },
        { x: -0.3276621465977682, y: 5.394144170850701 },
        { x: -0.39210815313215763, y: 5.910266003529118 },
        { x: -1.4635149481321186, y: 2.8021138454680625 },
        { x: 0.29612027706457605, y: 7.446432113093161 },
        { x: 0.26105527217988933, y: 7.860028369512432 },
        { x: 0.00511345664246089, y: 7.044444729150383 },
        { x: -0.23458713337514692, y: 5.724753450959248 }
    ];

    const X = syntheticData.map(p => [p.x]);
    const y = syntheticData.map(p => [p.y]);
    const m = y.length;

    // Matrix operations
    const matMul = (A, B) => A.map((row, i) => B[0].map((_, j) => row.reduce((sum, elm, k) => sum + elm * B[k][j], 0)));
    const transpose = A => A[0].map((_, colIndex) => A.map(row => row[colIndex]));
    
    // --- NEURAL NETWORK ANIMATION ---
    const canvas = document.getElementById('neuralNetCanvas');
    const ctx = canvas.getContext('2d');
    
    canvas.width = canvas.offsetWidth;
    canvas.height = 380;
    
    // Network architecture
    const layers = [3, 5, 5, 2];
    const nodes = [];
    const connections = [];
    
    // Initialize nodes
    const layerSpacing = canvas.width / (layers.length + 1);
    layers.forEach((count, layerIdx) => {
        const layerNodes = [];
        const verticalSpacing = canvas.height / (count + 1);
        for (let i = 0; i < count; i++) {
            layerNodes.push({
                x: layerSpacing * (layerIdx + 1),
                y: verticalSpacing * (i + 1),
                activation: Math.random() * 0.5 + 0.25
            });
        }
        nodes.push(layerNodes);
    });
    
    // Initialize connections
    for (let l = 0; l < layers.length - 1; l++) {
        for (let i = 0; i < nodes[l].length; i++) {
            for (let j = 0; j < nodes[l + 1].length; j++) {
                connections.push({
                    from: nodes[l][i],
                    to: nodes[l + 1][j],
                    weight: Math.random() * 2 - 1,
                    gradient: 0
                });
            }
        }
    }
    
    let animationFrame = 0;
    const maxFrames = 300;
    let loss = 1.0;
    
    function drawNeuralNetwork() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        // Draw connections
        connections.forEach(conn => {
            const gradient = Math.abs(conn.gradient);
            const alpha = Math.min(gradient * 1.5, 0.4);
            
            ctx.beginPath();
            ctx.moveTo(conn.from.x, conn.from.y);
            ctx.lineTo(conn.to.x, conn.to.y);
            ctx.strokeStyle = `rgba(88, 166, 255, ${alpha})`;
            ctx.lineWidth = Math.max(0.5, gradient * 2);
            ctx.stroke();
        });
        
        // Draw nodes
        nodes.forEach((layer, layerIdx) => {
            layer.forEach(node => {
                const radius = 10;
                const gradient = ctx.createRadialGradient(node.x, node.y, 0, node.x, node.y, radius);
                
                const intensity = node.activation;
                gradient.addColorStop(0, `rgba(88, 166, 255, ${0.6 + intensity * 0.4})`);
                gradient.addColorStop(1, `rgba(88, 166, 255, ${0.2 + intensity * 0.2})`);
                
                ctx.beginPath();
                ctx.arc(node.x, node.y, radius, 0, Math.PI * 2);
                ctx.fillStyle = gradient;
                ctx.fill();
                ctx.strokeStyle = 'rgba(88, 166, 255, 0.3)';
                ctx.lineWidth = 1.5;
                ctx.stroke();
            });
        });
        
        // Draw labels
        ctx.font = '600 12px Inter';
        ctx.fillStyle = 'rgba(139, 148, 158, 0.8)';
        ctx.textAlign = 'center';
        const labels = ['Input', 'Hidden 1', 'Hidden 2', 'Output'];
        layers.forEach((_, idx) => {
            ctx.fillText(labels[idx], layerSpacing * (idx + 1), 25);
        });
        
        // Draw metrics
        ctx.font = '500 14px Inter';
        ctx.fillStyle = 'rgba(88, 166, 255, 0.8)';
        ctx.textAlign = 'right';
        ctx.fillText(`Loss: ${loss.toFixed(4)}`, canvas.width - 20, canvas.height - 40);
        
        ctx.fillStyle = 'rgba(139, 148, 158, 0.6)';
        ctx.fillText(`Iteration: ${animationFrame}/${maxFrames}`, canvas.width - 20, canvas.height - 20);
    }
    
    function animateGradientDescent() {
        if (animationFrame < maxFrames) {
            // Update activations
            for (let l = 1; l < nodes.length; l++) {
                nodes[l].forEach(node => {
                    node.activation = Math.max(0.1, Math.min(0.9, 
                        node.activation + (Math.random() - 0.5) * 0.08
                    ));
                });
            }
            
            // Update gradients
            connections.forEach(conn => {
                conn.gradient = Math.abs(Math.sin(animationFrame * 0.08 + conn.weight * 2));
                conn.weight -= 0.008 * conn.gradient;
            });
            
            // Decay loss
            loss = Math.max(0.01, loss * 0.985);
            
            drawNeuralNetwork();
            animationFrame++;
            requestAnimationFrame(animateGradientDescent);
        } else {
            animationFrame = 0;
            loss = 1.0;
            animateGradientDescent();
        }
    }
    
    animateGradientDescent();
    
    // --- OPTIMIZER IMPLEMENTATIONS ---
    const train_sgd = (X, y, lr = 0.01, epochs = 100) => {
        let w = [[0]];
        let b = 0;
        const losses = [];

        for (let i = 0; i < epochs; i++) {
            const y_pred = matMul(X, w).map(row => row[0] + b);
            const error = y_pred.map((yp, j) => yp - y[j][0]);
            const loss = error.reduce((sum, e) => sum + e * e, 0) / m;
            losses.push(loss);

            const dw_mat = matMul(transpose(X), error.map(e => [e]));
            const dw = dw_mat[0][0] * (2 / m);
            const db = error.reduce((sum, e) => sum + e, 0) * (2 / m);

            w[0][0] -= lr * dw;
            b -= lr * db;
        }
        return losses;
    };
    
    const train_sgd_decay = (X, y, lr_initial = 0.1, decay = 0.05, epochs = 100) => {
        let w = [[0]];
        let b = 0;
        const losses = [];
        
        for (let i = 0; i < epochs; i++) {
            const lr = lr_initial / (1 + decay * i);
            const y_pred = matMul(X, w).map(row => row[0] + b);
            const error = y_pred.map((yp, j) => yp - y[j][0]);
            const loss = error.reduce((sum, e) => sum + e * e, 0) / m;
            losses.push(loss);

            const dw = matMul(transpose(X), error.map(e => [e]))[0][0] * (2 / m);
            const db = error.reduce((sum, e) => sum + e, 0) * (2 / m);
            
            w[0][0] -= lr * dw;
            b -= lr * db;
        }
        return losses;
    };

    const train_sgd_momentum = (X, y, lr = 0.01, momentum = 0.9, epochs = 100) => {
        let w = [[0]];
        let b = 0;
        let vw = 0, vb = 0;
        const losses = [];

        for (let i = 0; i < epochs; i++) {
            const y_pred = matMul(X, w).map(row => row[0] + b);
            const error = y_pred.map((yp, j) => yp - y[j][0]);
            const loss = error.reduce((sum, e) => sum + e * e, 0) / m;
            losses.push(loss);

            const dw = matMul(transpose(X), error.map(e => [e]))[0][0] * (2 / m);
            const db = error.reduce((sum, e) => sum + e, 0) * (2 / m);

            vw = momentum * vw + (1 - momentum) * dw;
            vb = momentum * vb + (1 - momentum) * db;

            w[0][0] -= lr * vw;
            b -= lr * vb;
        }
        return losses;
    };
    
    const train_adagrad = (X, y, lr = 0.1, epochs = 100, epsilon = 1e-8) => {
        let w = [[0]];
        let b = 0;
        let gw = 0, gb = 0;
        const losses = [];
        
        for (let i = 0; i < epochs; i++) {
            const y_pred = matMul(X, w).map(row => row[0] + b);
            const error = y_pred.map((yp, j) => yp - y[j][0]);
            losses.push(error.reduce((sum, e) => sum + e*e, 0) / m);

            const dw = matMul(transpose(X), error.map(e => [e]))[0][0] * (2 / m);
            const db = error.reduce((sum, e) => sum + e, 0) * (2 / m);
            
            gw += dw * dw;
            gb += db * db;
            
            w[0][0] -= (lr / (Math.sqrt(gw) + epsilon)) * dw;
            b -= (lr / (Math.sqrt(gb) + epsilon)) * db;
        }
        return losses;
    };

    const train_rmsprop = (X, y, lr = 0.01, beta = 0.9, epochs = 100, epsilon = 1e-8) => {
        let w = [[0]];
        let b = 0;
        let gw = 0, gb = 0;
        const losses = [];
        
        for (let i = 0; i < epochs; i++) {
            const y_pred = matMul(X, w).map(row => row[0] + b);
            const error = y_pred.map((yp, j) => yp - y[j][0]);
            losses.push(error.reduce((sum, e) => sum + e*e, 0) / m);

            const dw = matMul(transpose(X), error.map(e => [e]))[0][0] * (2 / m);
            const db = error.reduce((sum, e) => sum + e, 0) * (2 / m);
            
            gw = beta * gw + (1 - beta) * (dw * dw);
            gb = beta * gb + (1 - beta) * (db * db);
            
            w[0][0] -= (lr / (Math.sqrt(gw) + epsilon)) * dw;
            b -= (lr / (Math.sqrt(gb) + epsilon)) * db;
        }
        return losses;
    };

    const train_adam = (X, y, lr = 0.001, epochs = 100, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8) => {
        let w = [[0]];
        let b = 0;
        const losses = [];
        let mw = 0, vw = 0, mb = 0, vb = 0;

        for (let t = 1; t <= epochs; t++) {
            const y_pred = matMul(X, w).map(row => row[0] + b);
            const error = y_pred.map((yp, j) => yp - y[j][0]);
            losses.push(error.reduce((sum, e) => sum + e*e, 0) / m);

            const dw = matMul(transpose(X), error.map(e => [e]))[0][0] * (2 / m);
            const db = error.reduce((sum, e) => sum + e, 0) * (2 / m);
            
            mw = beta1 * mw + (1 - beta1) * dw;
            vw = beta2 * vw + (1 - beta2) * (dw * dw);
            mb = beta1 * mb + (1 - beta1) * db;
            vb = beta2 * vb + (1 - beta2) * (db * db);
            
            const mw_hat = mw / (1 - Math.pow(beta1, t));
            const vw_hat = vw / (1 - Math.pow(beta2, t));
            const mb_hat = mb / (1 - Math.pow(beta1, t));
            const vb_hat = vb / (1 - Math.pow(beta2, t));
            
            w[0][0] -= (lr * mw_hat) / (Math.sqrt(vw_hat) + epsilon);
            b -= (lr * mb_hat) / (Math.sqrt(vb_hat) + epsilon);
        }
        return losses;
    };
    
    // --- CHART CONFIGURATION ---
    const chartOptions = {
        responsive: true,
        maintainAspectRatio: true,
        plugins: {
            legend: { 
                labels: { 
                    color: '#c9d1d9',
                    font: { size: 11, family: 'Inter' }
                }
            },
            tooltip: {
                backgroundColor: 'rgba(13, 17, 23, 0.95)',
                titleColor: '#58a6ff',
                bodyColor: '#c9d1d9',
                borderColor: 'rgba(88, 166, 255, 0.3)',
                borderWidth: 1
            }
        },
        scales: {
            x: {
                title: { display: true, text: 'Epoch', color: '#8b949e', font: { size: 11 } },
                ticks: { color: '#8b949e', font: { size: 10 } },
                grid: { color: 'rgba(48, 54, 61, 0.3)' }
            },
            y: {
                title: { display: true, text: 'Loss', color: '#8b949e', font: { size: 11 } },
                ticks: { color: '#8b949e', font: { size: 10 } },
                grid: { color: 'rgba(48, 54, 61, 0.3)' }
            }
        },
        animation: {
            duration: 1500,
            easing: 'easeInOutCubic'
        }
    };
    
    const chartColors = ['#58a6ff', '#7ee787', '#a371f7', '#ffa657', '#f85149', '#ff7b72'];

    // Dataset Chart
    const datasetCtx = document.getElementById('datasetChart').getContext('2d');
    new Chart(datasetCtx, {
        type: 'scatter',
        data: {
            datasets: [{
                label: 'Training Data',
                data: syntheticData,
                backgroundColor: 'rgba(88, 166, 255, 0.5)',
                borderColor: '#58a6ff',
                pointRadius: 4,
                pointHoverRadius: 6
            }, {
                label: 'True Relationship',
                data: syntheticData.map(p => ({ x: p.x, y: 3 * p.x + 7 })).sort((a, b) => a.x - b.x),
                type: 'line',
                borderColor: 'rgba(126, 231, 135, 0.7)',
                backgroundColor: 'transparent',
                pointRadius: 0,
                borderWidth: 2,
                borderDash: [4, 4]
            }]
        },
        options: {
            ...chartOptions,
            scales: {
                x: { 
                    title: { display: true, text: 'Features (X)', color: '#8b949e' }, 
                    ticks: { color: '#8b949e' }, 
                    grid: { color: 'rgba(48, 54, 61, 0.3)' }
                },
                y: { 
                    title: { display: true, text: 'Target (y)', color: '#8b949e' }, 
                    ticks: { color: '#8b949e' }, 
                    grid: { color: 'rgba(48, 54, 61, 0.3)' }
                }
            }
        }
    });

    // SGD Chart
    const sgdLosses = train_sgd(X, y);
    const sgdCtx = document.getElementById('sgdChart').getContext('2d');
    new Chart(sgdCtx, {
        type: 'line',
        data: {
            labels: Array.from({ length: sgdLosses.length }, (_, i) => i + 1),
            datasets: [{ 
                label: 'SGD', 
                data: sgdLosses, 
                borderColor: chartColors[0], 
                backgroundColor: 'rgba(88, 166, 255, 0.1)',
                tension: 0.3, 
                fill: true,
                borderWidth: 2
            }]
        },
        options: chartOptions
    });
    
    // Decay Chart
    const decayLosses = train_sgd_decay(X, y);
    const decayCtx = document.getElementById('decayChart').getContext('2d');
    new Chart(decayCtx, {
        type: 'line',
        data: {
            labels: Array.from({ length: decayLosses.length }, (_, i) => i + 1),
            datasets: [
                { label: 'SGD', data: sgdLosses, borderColor: chartColors[0], tension: 0.3, borderWidth: 1.5 },
                { label: 'SGD + Decay', data: decayLosses, borderColor: chartColors[1], tension: 0.3, borderWidth: 2 }
            ]
        },
        options: chartOptions
    });

    // Momentum Chart
    const momentumLosses = {
        '0': train_sgd_momentum(X, y, 0.01, 0),
        '0.5': train_sgd_momentum(X, y, 0.01, 0.5),
        '0.9': train_sgd_momentum(X, y, 0.01, 0.9),
        '0.99': train_sgd_momentum(X, y, 0.01, 0.99),
    };
    const momentumCtx = document.getElementById('momentumChart').getContext('2d');
    new Chart(momentumCtx, {
        type: 'line',
        data: {
            labels: Array.from({ length: 100 }, (_, i) => i + 1),
            datasets: Object.keys(momentumLosses).map((key, i) => ({
                label: `β=${key}`,
                data: momentumLosses[key],
                borderColor: chartColors[i],
                tension: 0.3,
                borderWidth: 2
            }))
        },
        options: chartOptions
    });
    
    // AdaGrad Chart
    const adagradLosses = train_adagrad(X, y);
    const adagradCtx = document.getElementById('adagradChart').getContext('2d');
    new Chart(adagradCtx, {
        type: 'line',
        data: {
            labels: Array.from({ length: 100 }, (_, i) => i + 1),
            datasets: [
                { label: 'SGD', data: sgdLosses, borderColor: chartColors[0], tension: 0.3, borderWidth: 1.5 },
                { label: 'AdaGrad', data: adagradLosses, borderColor: chartColors[3], tension: 0.3, borderWidth: 2 }
            ]
        },
        options: chartOptions
    });

    // RMSProp Chart
    const rmspropLosses = train_rmsprop(X, y);
    const rmspropCtx = document.getElementById('rmspropChart').getContext('2d');
    new Chart(rmspropCtx, {
        type: 'line',
        data: {
            labels: Array.from({ length: 100 }, (_, i) => i + 1),
            datasets: [
                { label: 'SGD', data: sgdLosses, borderColor: chartColors[0], tension: 0.3, borderWidth: 1.5 },
                { label: 'AdaGrad', data: adagradLosses, borderColor: chartColors[3], tension: 0.3, borderWidth: 1.5 },
                { label: 'RMSProp', data: rmspropLosses, borderColor: chartColors[4], tension: 0.3, borderWidth: 2 }
            ]
        },
        options: chartOptions
    });
    
    // Adam Chart
    const adamLosses = train_adam(X, y);
    const adamCtx = document.getElementById('adamChart').getContext('2d');
    new Chart(adamCtx, {
        type: 'line',
        data: {
            labels: Array.from({ length: 100 }, (_, i) => i + 1),
            datasets: [
                { label: 'SGD', data: sgdLosses, borderColor: chartColors[0], tension: 0.3, borderWidth: 1.5 },
                { label: 'Momentum (β=0.9)', data: momentumLosses['0.9'], borderColor: chartColors[2], tension: 0.3, borderWidth: 1.5 },
                { label: 'RMSProp', data: rmspropLosses, borderColor: chartColors[4], tension: 0.3, borderWidth: 1.5 },
                { label: 'Adam', data: adamLosses, borderColor: chartColors[5], tension: 0.3, borderWidth: 2.5 }
            ]
        },
        options: chartOptions
    });

    // Copy button functionality
    document.querySelectorAll('.copy-btn').forEach(button => {
        button.addEventListener('click', (e) => {
            const code = e.target.previousElementSibling.innerText;
            navigator.clipboard.writeText(code).then(() => {
                const originalText = button.innerText;
                button.innerText = 'Copied';
                button.style.background = 'rgba(126, 231, 135, 0.2)';
                button.style.borderColor = 'rgba(126, 231, 135, 0.5)';
                setTimeout(() => { 
                    button.innerText = originalText;
                    button.style.background = 'rgba(48, 54, 61, 0.8)';
                    button.style.borderColor = 'rgba(88, 166, 255, 0.3)';
                }, 1800);
            });
        });
    });

    // Scroll animations
    const sections = document.querySelectorAll('.fade-in-section');
    const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                entry.target.classList.add('is-visible');
            }
        });
    }, {
        rootMargin: '0px',
        threshold: 0.1
    });

    sections.forEach(section => {
        observer.observe(section);
    });
    
    // Smooth scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
            e.preventDefault();
            const target = document.querySelector(this.getAttribute('href'));
            if (target) {
                target.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        });
    });
});
</script>
</body>
</html>